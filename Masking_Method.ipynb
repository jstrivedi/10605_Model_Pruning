{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masking Method.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MCDbLpE-9z7",
        "outputId": "3128151a-6de4-4277-d9d1-7ee0d5a63019"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models, regularizers\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(tf.version.VERSION)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjoDVgzs_RDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4799ff2-83f5-4dde-c8d5-32dd5281e1a6"
      },
      "source": [
        "# untar\n",
        "!tar -xvzf dataset.tar.gz\n",
        "# load train\n",
        "train_images = pickle.load(open('train_images.pkl', 'rb'))\n",
        "train_labels = pickle.load(open('train_labels.pkl', 'rb'))\n",
        "# load val\n",
        "val_images = pickle.load(open('val_images.pkl', 'rb'))\n",
        "val_labels = pickle.load(open('val_labels.pkl', 'rb'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_images.pkl\n",
            "train_labels.pkl\n",
            "val_images.pkl\n",
            "val_labels.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lrylVRy_RMP"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5), input_shape=(25,25,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQifTiJQHY6p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dcfdb86-d3bb-426f-b76f-270a60989f31"
      },
      "source": [
        "BEST_RESULT_WEIGHTS = 'my_model_weights.h5'\n",
        "\n",
        "model.load_weights(BEST_RESULT_WEIGHTS) # Load the best weight we had from the previous prunings\n",
        "def sparsify(model,t):\n",
        "    tr_weights = np.copy(model.get_weights())\n",
        "    stacked_wts = np.hstack([i.flatten() for i in tr_weights])\n",
        "    thres = np.percentile(np.abs(stacked_wts),t)\n",
        "    for i in range(len(tr_weights)):\n",
        "        tr = np.abs(tr_weights[i])>thres\n",
        "        tr_weights[i] = np.multiply(tr_weights[i],tr)\n",
        "    return tr_weights\n",
        "\n",
        "t=97\n",
        "tr_weights = sparsify(model,t)\n",
        "model.set_weights(tr_weights)\n",
        "\n",
        "\n",
        "# Remasking method\n",
        "initWs = [tf.identity(W) for W in model.trainable_variables]\n",
        "\n",
        "# The following code for class definition is modelled after: \n",
        "# https://keras.io/guides/customizing_what_happens_in_fit/\n",
        "class MaskModel(models.Sequential):\n",
        "  '''\n",
        "  A Keras model extended from the Sequential model, such that the per-epoch\n",
        "  training is overwritten so that the gradient descent would only update the weights \n",
        "  initially set to nonzeros\n",
        "  '''\n",
        "  def __init__(self, initWs):\n",
        "    '''\n",
        "    initWs: List[Tensor], the initial best weights Saurabh acheievd\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.mask = [tf.cast(initW!=0, tf.float32) for initW in initWs]\n",
        "  \n",
        "  def train_step(self, data):\n",
        "    '''\n",
        "    the per-epoch traning we overwrite\n",
        "    '''\n",
        "    x, y = data\n",
        "\n",
        "    with tf.GradientTape() as tape: \n",
        "      y_pred = self(x, training=True)\n",
        "      loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)   \n",
        "\n",
        "    # Compute gradients and mask out the weights that should remain zero\n",
        "    gradients = [ g*m for g, m in zip(tape.gradient(loss, self.trainable_variables), self.mask)]\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    # Update and return metrics \n",
        "    self.compiled_metrics.update_state(y, y_pred)\n",
        "    return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "model = MaskModel(initWs)\n",
        "model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5), input_shape=(25,25,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('softmax'))\n",
        "# model.load_weights(BEST_RESULT_WEIGHTS) # Again. load the best weight we had from the previous prunings\n",
        "model.set_weights(tr_weights)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:792: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, order=order, subok=subok, copy=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_57laoByDHJ",
        "outputId": "777eb4f4-b96a-4da9-ab4d-8fe4293de3bd"
      },
      "source": [
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001, decay=1e-6),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "def getSparse(model):\n",
        "  return np.sum(np.hstack([k.flatten() for k in model.get_weights()])==0)/np.shape(np.hstack([k.flatten() for k in model.get_weights()]))[0]\n",
        "def getAccu(model):\n",
        "  return model.evaluate(val_images, val_labels, batch_size=128)[1]\n",
        "print(getSparse(model), getAccu(model))\n",
        "\n",
        "# Train as many epoches as possible, until we have a very good validation accuracy (e.g. 75%+)\n",
        "# model.fit(x = train_images, y = train_labels,batch_size=64, epochs=10,validation_data=(val_images, val_labels))\n",
        "################################################################################NEW\n",
        "checkpoint_filepath = '/tmp/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=40)\n",
        "\n",
        "model.fit(x = train_images, y = train_labels,batch_size=128, epochs=200,validation_data=(val_images, val_labels),callbacks=[early_stopping, model_checkpoint_callback])##ADDED Callbacks\n",
        "model.load_weights(checkpoint_filepath)##model will load the checkpointed model weights with best val_accuracy\n",
        "################################################################################NEW"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 2s 75ms/step - loss: 0.7029 - accuracy: 0.7469\n",
            "0.969999983134688 0.746930718421936\n",
            "Epoch 1/200\n",
            "176/176 [==============================] - 54s 300ms/step - loss: 0.7130 - accuracy: 0.7342 - val_loss: 0.6985 - val_accuracy: 0.7442\n",
            "Epoch 2/200\n",
            "176/176 [==============================] - 52s 296ms/step - loss: 0.7077 - accuracy: 0.7325 - val_loss: 0.6992 - val_accuracy: 0.7438\n",
            "Epoch 3/200\n",
            "176/176 [==============================] - 53s 298ms/step - loss: 0.7113 - accuracy: 0.7298 - val_loss: 0.7053 - val_accuracy: 0.7406\n",
            "Epoch 4/200\n",
            "176/176 [==============================] - 53s 304ms/step - loss: 0.7149 - accuracy: 0.7276 - val_loss: 0.7102 - val_accuracy: 0.7358\n",
            "Epoch 5/200\n",
            "176/176 [==============================] - 53s 300ms/step - loss: 0.7048 - accuracy: 0.7320 - val_loss: 0.7072 - val_accuracy: 0.7414\n",
            "Epoch 6/200\n",
            "176/176 [==============================] - 52s 298ms/step - loss: 0.7065 - accuracy: 0.7354 - val_loss: 0.7047 - val_accuracy: 0.7426\n",
            "Epoch 7/200\n",
            "176/176 [==============================] - 53s 300ms/step - loss: 0.7119 - accuracy: 0.7286 - val_loss: 0.7087 - val_accuracy: 0.7402\n",
            "Epoch 8/200\n",
            "176/176 [==============================] - 53s 300ms/step - loss: 0.7061 - accuracy: 0.7308 - val_loss: 0.7098 - val_accuracy: 0.7414\n",
            "Epoch 9/200\n",
            "176/176 [==============================] - 53s 299ms/step - loss: 0.7121 - accuracy: 0.7319 - val_loss: 0.7058 - val_accuracy: 0.7410\n",
            "Epoch 10/200\n",
            "176/176 [==============================] - 54s 305ms/step - loss: 0.7066 - accuracy: 0.7360 - val_loss: 0.7044 - val_accuracy: 0.7366\n",
            "Epoch 11/200\n",
            "176/176 [==============================] - 53s 303ms/step - loss: 0.7152 - accuracy: 0.7304 - val_loss: 0.7050 - val_accuracy: 0.7378\n",
            "Epoch 12/200\n",
            "176/176 [==============================] - 54s 304ms/step - loss: 0.7061 - accuracy: 0.7355 - val_loss: 0.7026 - val_accuracy: 0.7430\n",
            "Epoch 13/200\n",
            "176/176 [==============================] - 53s 300ms/step - loss: 0.7037 - accuracy: 0.7336 - val_loss: 0.6979 - val_accuracy: 0.7438\n",
            "Epoch 14/200\n",
            "176/176 [==============================] - 53s 300ms/step - loss: 0.7103 - accuracy: 0.7324 - val_loss: 0.6981 - val_accuracy: 0.7434\n",
            "Epoch 15/200\n",
            "176/176 [==============================] - 53s 301ms/step - loss: 0.6995 - accuracy: 0.7369 - val_loss: 0.6982 - val_accuracy: 0.7402\n",
            "Epoch 16/200\n",
            "176/176 [==============================] - 53s 303ms/step - loss: 0.7076 - accuracy: 0.7307 - val_loss: 0.7024 - val_accuracy: 0.7418\n",
            "Epoch 17/200\n",
            "176/176 [==============================] - 53s 300ms/step - loss: 0.7074 - accuracy: 0.7318 - val_loss: 0.7030 - val_accuracy: 0.7422\n",
            "Epoch 18/200\n",
            "176/176 [==============================] - 53s 299ms/step - loss: 0.7067 - accuracy: 0.7373 - val_loss: 0.6989 - val_accuracy: 0.7442\n",
            "Epoch 19/200\n",
            "176/176 [==============================] - 53s 302ms/step - loss: 0.7058 - accuracy: 0.7350 - val_loss: 0.7010 - val_accuracy: 0.7422\n",
            "Epoch 20/200\n",
            "176/176 [==============================] - 53s 304ms/step - loss: 0.7052 - accuracy: 0.7336 - val_loss: 0.7065 - val_accuracy: 0.7362\n",
            "Epoch 21/200\n",
            "176/176 [==============================] - 53s 304ms/step - loss: 0.7118 - accuracy: 0.7315 - val_loss: 0.7011 - val_accuracy: 0.7422\n",
            "Epoch 22/200\n",
            "176/176 [==============================] - 53s 302ms/step - loss: 0.7036 - accuracy: 0.7369 - val_loss: 0.7101 - val_accuracy: 0.7362\n",
            "Epoch 23/200\n",
            "176/176 [==============================] - 53s 302ms/step - loss: 0.7087 - accuracy: 0.7322 - val_loss: 0.7125 - val_accuracy: 0.7382\n",
            "Epoch 24/200\n",
            "176/176 [==============================] - 53s 299ms/step - loss: 0.7082 - accuracy: 0.7304 - val_loss: 0.7079 - val_accuracy: 0.7402\n",
            "Epoch 25/200\n",
            "176/176 [==============================] - 53s 302ms/step - loss: 0.7035 - accuracy: 0.7346 - val_loss: 0.7017 - val_accuracy: 0.7382\n",
            "Epoch 26/200\n",
            "176/176 [==============================] - 53s 300ms/step - loss: 0.7083 - accuracy: 0.7317 - val_loss: 0.6944 - val_accuracy: 0.7446\n",
            "Epoch 27/200\n",
            "176/176 [==============================] - 53s 301ms/step - loss: 0.7091 - accuracy: 0.7306 - val_loss: 0.7052 - val_accuracy: 0.7410\n",
            "Epoch 28/200\n",
            "176/176 [==============================] - 53s 303ms/step - loss: 0.7037 - accuracy: 0.7354 - val_loss: 0.6964 - val_accuracy: 0.7442\n",
            "Epoch 29/200\n",
            "176/176 [==============================] - 53s 301ms/step - loss: 0.7060 - accuracy: 0.7359 - val_loss: 0.7028 - val_accuracy: 0.7434\n",
            "Epoch 30/200\n",
            "176/176 [==============================] - 53s 298ms/step - loss: 0.7050 - accuracy: 0.7337 - val_loss: 0.7024 - val_accuracy: 0.7434\n",
            "Epoch 31/200\n",
            "176/176 [==============================] - 53s 299ms/step - loss: 0.7004 - accuracy: 0.7362 - val_loss: 0.6987 - val_accuracy: 0.7414\n",
            "Epoch 32/200\n",
            "176/176 [==============================] - 53s 301ms/step - loss: 0.7089 - accuracy: 0.7328 - val_loss: 0.7067 - val_accuracy: 0.7402\n",
            "Epoch 33/200\n",
            "176/176 [==============================] - 53s 300ms/step - loss: 0.7006 - accuracy: 0.7362 - val_loss: 0.6993 - val_accuracy: 0.7446\n",
            "Epoch 34/200\n",
            "176/176 [==============================] - 53s 302ms/step - loss: 0.7016 - accuracy: 0.7334 - val_loss: 0.7006 - val_accuracy: 0.7382\n",
            "Epoch 35/200\n",
            "176/176 [==============================] - 53s 302ms/step - loss: 0.7069 - accuracy: 0.7341 - val_loss: 0.6987 - val_accuracy: 0.7414\n",
            "Epoch 36/200\n",
            "176/176 [==============================] - 53s 299ms/step - loss: 0.7051 - accuracy: 0.7312 - val_loss: 0.7070 - val_accuracy: 0.7386\n",
            "Epoch 37/200\n",
            "176/176 [==============================] - 53s 302ms/step - loss: 0.7003 - accuracy: 0.7348 - val_loss: 0.6944 - val_accuracy: 0.7422\n",
            "Epoch 38/200\n",
            "176/176 [==============================] - 53s 302ms/step - loss: 0.7019 - accuracy: 0.7370 - val_loss: 0.6992 - val_accuracy: 0.7390\n",
            "Epoch 39/200\n",
            "176/176 [==============================] - 53s 301ms/step - loss: 0.7038 - accuracy: 0.7331 - val_loss: 0.7007 - val_accuracy: 0.7434\n",
            "Epoch 40/200\n",
            "176/176 [==============================] - 53s 302ms/step - loss: 0.7064 - accuracy: 0.7326 - val_loss: 0.7030 - val_accuracy: 0.7394\n",
            "Epoch 41/200\n",
            "176/176 [==============================] - 53s 303ms/step - loss: 0.7152 - accuracy: 0.7329 - val_loss: 0.7086 - val_accuracy: 0.7378\n",
            "Epoch 42/200\n",
            "176/176 [==============================] - 53s 300ms/step - loss: 0.7078 - accuracy: 0.7345 - val_loss: 0.7041 - val_accuracy: 0.7390\n",
            "Epoch 43/200\n",
            "176/176 [==============================] - 53s 301ms/step - loss: 0.6958 - accuracy: 0.7358 - val_loss: 0.7073 - val_accuracy: 0.7406\n",
            "Epoch 44/200\n",
            "176/176 [==============================] - 53s 304ms/step - loss: 0.7032 - accuracy: 0.7346 - val_loss: 0.6987 - val_accuracy: 0.7453\n",
            "Epoch 45/200\n",
            "176/176 [==============================] - 53s 301ms/step - loss: 0.6935 - accuracy: 0.7388 - val_loss: 0.6992 - val_accuracy: 0.7382\n",
            "Epoch 46/200\n",
            "176/176 [==============================] - 53s 299ms/step - loss: 0.6990 - accuracy: 0.7369 - val_loss: 0.7069 - val_accuracy: 0.7374\n",
            "Epoch 47/200\n",
            "176/176 [==============================] - 52s 298ms/step - loss: 0.7044 - accuracy: 0.7322 - val_loss: 0.7020 - val_accuracy: 0.7382\n",
            "Epoch 48/200\n",
            "176/176 [==============================] - 53s 299ms/step - loss: 0.7035 - accuracy: 0.7356 - val_loss: 0.6987 - val_accuracy: 0.7438\n",
            "Epoch 49/200\n",
            "176/176 [==============================] - 52s 296ms/step - loss: 0.6997 - accuracy: 0.7317 - val_loss: 0.6979 - val_accuracy: 0.7434\n",
            "Epoch 50/200\n",
            "176/176 [==============================] - 52s 298ms/step - loss: 0.7009 - accuracy: 0.7359 - val_loss: 0.7004 - val_accuracy: 0.7422\n",
            "Epoch 51/200\n",
            "176/176 [==============================] - 52s 295ms/step - loss: 0.7005 - accuracy: 0.7351 - val_loss: 0.6951 - val_accuracy: 0.7406\n",
            "Epoch 52/200\n",
            "176/176 [==============================] - 53s 300ms/step - loss: 0.7029 - accuracy: 0.7352 - val_loss: 0.6958 - val_accuracy: 0.7426\n",
            "Epoch 53/200\n",
            "176/176 [==============================] - 54s 304ms/step - loss: 0.7016 - accuracy: 0.7342 - val_loss: 0.7022 - val_accuracy: 0.7378\n",
            "Epoch 54/200\n",
            "176/176 [==============================] - 54s 305ms/step - loss: 0.7033 - accuracy: 0.7330 - val_loss: 0.6961 - val_accuracy: 0.7422\n",
            "Epoch 55/200\n",
            "176/176 [==============================] - 54s 308ms/step - loss: 0.6948 - accuracy: 0.7367 - val_loss: 0.6960 - val_accuracy: 0.7434\n",
            "Epoch 56/200\n",
            "176/176 [==============================] - 55s 310ms/step - loss: 0.6895 - accuracy: 0.7414 - val_loss: 0.7013 - val_accuracy: 0.7430\n",
            "Epoch 57/200\n",
            "176/176 [==============================] - 54s 309ms/step - loss: 0.7026 - accuracy: 0.7336 - val_loss: 0.7010 - val_accuracy: 0.7414\n",
            "Epoch 58/200\n",
            "176/176 [==============================] - 54s 308ms/step - loss: 0.6984 - accuracy: 0.7367 - val_loss: 0.7036 - val_accuracy: 0.7390\n",
            "Epoch 59/200\n",
            "176/176 [==============================] - 54s 309ms/step - loss: 0.7136 - accuracy: 0.7337 - val_loss: 0.6938 - val_accuracy: 0.7426\n",
            "Epoch 60/200\n",
            "176/176 [==============================] - 54s 307ms/step - loss: 0.6980 - accuracy: 0.7370 - val_loss: 0.6988 - val_accuracy: 0.7457\n",
            "Epoch 61/200\n",
            "176/176 [==============================] - 54s 309ms/step - loss: 0.7007 - accuracy: 0.7375 - val_loss: 0.7005 - val_accuracy: 0.7410\n",
            "Epoch 62/200\n",
            "176/176 [==============================] - 55s 314ms/step - loss: 0.7000 - accuracy: 0.7400 - val_loss: 0.6917 - val_accuracy: 0.7461\n",
            "Epoch 63/200\n",
            "176/176 [==============================] - 55s 315ms/step - loss: 0.7008 - accuracy: 0.7362 - val_loss: 0.7026 - val_accuracy: 0.7370\n",
            "Epoch 64/200\n",
            "176/176 [==============================] - 55s 310ms/step - loss: 0.6968 - accuracy: 0.7381 - val_loss: 0.6995 - val_accuracy: 0.7390\n",
            "Epoch 65/200\n",
            "176/176 [==============================] - 54s 307ms/step - loss: 0.6909 - accuracy: 0.7372 - val_loss: 0.7031 - val_accuracy: 0.7370\n",
            "Epoch 66/200\n",
            "176/176 [==============================] - 54s 308ms/step - loss: 0.6982 - accuracy: 0.7363 - val_loss: 0.7050 - val_accuracy: 0.7370\n",
            "Epoch 67/200\n",
            "176/176 [==============================] - 54s 308ms/step - loss: 0.6989 - accuracy: 0.7369 - val_loss: 0.7105 - val_accuracy: 0.7366\n",
            "Epoch 68/200\n",
            "176/176 [==============================] - 54s 309ms/step - loss: 0.7018 - accuracy: 0.7346 - val_loss: 0.7027 - val_accuracy: 0.7382\n",
            "Epoch 69/200\n",
            "176/176 [==============================] - 54s 308ms/step - loss: 0.7039 - accuracy: 0.7349 - val_loss: 0.6974 - val_accuracy: 0.7442\n",
            "Epoch 70/200\n",
            "176/176 [==============================] - 55s 314ms/step - loss: 0.7018 - accuracy: 0.7362 - val_loss: 0.6971 - val_accuracy: 0.7434\n",
            "Epoch 71/200\n",
            "176/176 [==============================] - 57s 322ms/step - loss: 0.6961 - accuracy: 0.7366 - val_loss: 0.7013 - val_accuracy: 0.7422\n",
            "Epoch 72/200\n",
            "176/176 [==============================] - 61s 345ms/step - loss: 0.7003 - accuracy: 0.7376 - val_loss: 0.6991 - val_accuracy: 0.7438\n",
            "Epoch 73/200\n",
            "176/176 [==============================] - 61s 348ms/step - loss: 0.6970 - accuracy: 0.7366 - val_loss: 0.7044 - val_accuracy: 0.7422\n",
            "Epoch 74/200\n",
            "176/176 [==============================] - 68s 388ms/step - loss: 0.6996 - accuracy: 0.7370 - val_loss: 0.6963 - val_accuracy: 0.7406\n",
            "Epoch 75/200\n",
            "176/176 [==============================] - 60s 339ms/step - loss: 0.6989 - accuracy: 0.7363 - val_loss: 0.7019 - val_accuracy: 0.7402\n",
            "Epoch 76/200\n",
            "176/176 [==============================] - 60s 340ms/step - loss: 0.7015 - accuracy: 0.7375 - val_loss: 0.7087 - val_accuracy: 0.7366\n",
            "Epoch 77/200\n",
            "176/176 [==============================] - 61s 347ms/step - loss: 0.6966 - accuracy: 0.7378 - val_loss: 0.6986 - val_accuracy: 0.7426\n",
            "Epoch 78/200\n",
            "176/176 [==============================] - 56s 321ms/step - loss: 0.6990 - accuracy: 0.7382 - val_loss: 0.6951 - val_accuracy: 0.7442\n",
            "Epoch 79/200\n",
            "176/176 [==============================] - 54s 307ms/step - loss: 0.7001 - accuracy: 0.7384 - val_loss: 0.6982 - val_accuracy: 0.7410\n",
            "Epoch 80/200\n",
            "176/176 [==============================] - 54s 307ms/step - loss: 0.7013 - accuracy: 0.7337 - val_loss: 0.6973 - val_accuracy: 0.7410\n",
            "Epoch 81/200\n",
            "176/176 [==============================] - 54s 307ms/step - loss: 0.6988 - accuracy: 0.7356 - val_loss: 0.7010 - val_accuracy: 0.7398\n",
            "Epoch 82/200\n",
            "176/176 [==============================] - 55s 312ms/step - loss: 0.7032 - accuracy: 0.7373 - val_loss: 0.7044 - val_accuracy: 0.7350\n",
            "Epoch 83/200\n",
            "176/176 [==============================] - 55s 311ms/step - loss: 0.7009 - accuracy: 0.7342 - val_loss: 0.6967 - val_accuracy: 0.7461\n",
            "Epoch 84/200\n",
            "176/176 [==============================] - 55s 312ms/step - loss: 0.7036 - accuracy: 0.7361 - val_loss: 0.7027 - val_accuracy: 0.7426\n",
            "Epoch 85/200\n",
            "176/176 [==============================] - 55s 310ms/step - loss: 0.6959 - accuracy: 0.7381 - val_loss: 0.7044 - val_accuracy: 0.7414\n",
            "Epoch 86/200\n",
            "176/176 [==============================] - 54s 308ms/step - loss: 0.7012 - accuracy: 0.7375 - val_loss: 0.6980 - val_accuracy: 0.7418\n",
            "Epoch 87/200\n",
            "176/176 [==============================] - 54s 309ms/step - loss: 0.6897 - accuracy: 0.7381 - val_loss: 0.6976 - val_accuracy: 0.7461\n",
            "Epoch 88/200\n",
            "176/176 [==============================] - 56s 316ms/step - loss: 0.7015 - accuracy: 0.7346 - val_loss: 0.7066 - val_accuracy: 0.7358\n",
            "Epoch 89/200\n",
            "176/176 [==============================] - 56s 318ms/step - loss: 0.7000 - accuracy: 0.7364 - val_loss: 0.6990 - val_accuracy: 0.7434\n",
            "Epoch 90/200\n",
            "176/176 [==============================] - 54s 309ms/step - loss: 0.6946 - accuracy: 0.7350 - val_loss: 0.6963 - val_accuracy: 0.7398\n",
            "Epoch 91/200\n",
            "176/176 [==============================] - 55s 313ms/step - loss: 0.7055 - accuracy: 0.7345 - val_loss: 0.6975 - val_accuracy: 0.7418\n",
            "Epoch 92/200\n",
            "176/176 [==============================] - 56s 318ms/step - loss: 0.6988 - accuracy: 0.7358 - val_loss: 0.6960 - val_accuracy: 0.7434\n",
            "Epoch 93/200\n",
            "176/176 [==============================] - 58s 329ms/step - loss: 0.6959 - accuracy: 0.7365 - val_loss: 0.6967 - val_accuracy: 0.7426\n",
            "Epoch 94/200\n",
            "176/176 [==============================] - 56s 320ms/step - loss: 0.6965 - accuracy: 0.7371 - val_loss: 0.7031 - val_accuracy: 0.7394\n",
            "Epoch 95/200\n",
            "176/176 [==============================] - 56s 320ms/step - loss: 0.6976 - accuracy: 0.7360 - val_loss: 0.6978 - val_accuracy: 0.7402\n",
            "Epoch 96/200\n",
            "176/176 [==============================] - 56s 318ms/step - loss: 0.6970 - accuracy: 0.7363 - val_loss: 0.6924 - val_accuracy: 0.7446\n",
            "Epoch 97/200\n",
            "176/176 [==============================] - 55s 314ms/step - loss: 0.6994 - accuracy: 0.7357 - val_loss: 0.7020 - val_accuracy: 0.7414\n",
            "Epoch 98/200\n",
            "176/176 [==============================] - 54s 309ms/step - loss: 0.6924 - accuracy: 0.7397 - val_loss: 0.6921 - val_accuracy: 0.7446\n",
            "Epoch 99/200\n",
            "176/176 [==============================] - 54s 306ms/step - loss: 0.6994 - accuracy: 0.7345 - val_loss: 0.7030 - val_accuracy: 0.7390\n",
            "Epoch 100/200\n",
            "176/176 [==============================] - 54s 306ms/step - loss: 0.6973 - accuracy: 0.7390 - val_loss: 0.7034 - val_accuracy: 0.7370\n",
            "Epoch 101/200\n",
            "176/176 [==============================] - 54s 304ms/step - loss: 0.6931 - accuracy: 0.7397 - val_loss: 0.6992 - val_accuracy: 0.7410\n",
            "Epoch 102/200\n",
            "176/176 [==============================] - 55s 311ms/step - loss: 0.7007 - accuracy: 0.7322 - val_loss: 0.6935 - val_accuracy: 0.7450\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc313413990>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYbXn6M-_OyU",
        "outputId": "03fe2a88-5dda-44ed-b1c8-055c9b18db13"
      },
      "source": [
        "a,b = getSparse(model), getAccu(model)\n",
        "print(a,b,(a+b)/2)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 1s 74ms/step - loss: 0.6917 - accuracy: 0.7461\n",
            "0.969999983134688 0.7461386322975159 0.8580693077161019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "Vz4x4r_DI8U8",
        "outputId": "275b9e15-b6f1-458d-a513-1616af6cd8b0"
      },
      "source": [
        "model.save_weights(\"my_model_weights.h5\")\n",
        "files.download(\"my_model_weights.h5\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_278115e0-6abc-477a-b049-80ac4449cd4c\", \"my_model_weights.h5\", 2407560)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLvm5jwjv7n4"
      },
      "source": [
        "# TODO: Prepare data for frontier diagram"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}